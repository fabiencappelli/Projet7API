{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5drKlSgxUS5"
   },
   "source": [
    "# Imports et Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 33411,
     "status": "ok",
     "timestamp": 1747738487014,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "xPobTGhKv5qo",
    "outputId": "483f8a96-ca4f-4c3b-ce49-a9e6c89926bd"
   },
   "outputs": [],
   "source": [
    "!pip install mlflow dagshub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "executionInfo": {
     "elapsed": 2805,
     "status": "ok",
     "timestamp": 1747738634162,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "gX0vLl_Txm7n",
    "outputId": "2fac2497-1bca-4e71-f4e2-b89edb6a5680"
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import dagshub\n",
    "import mlflow.keras\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import time\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Récupère automatiquement le secret\n",
    "dagshub_token = userdata.get('DAGSHUB_TOKEN')\n",
    "\n",
    "# Initialisation Dagshub\n",
    "dagshub.auth.add_app_token(dagshub_token)\n",
    "\n",
    "# Connecter MLflow à Dagshub\n",
    "dagshub.init(repo_owner='fabiencappelli', repo_name='Projet_07', mlflow=True)\n",
    "\n",
    "# Configure MLflow pour pointer vers Dagshub\n",
    "mlflow.set_tracking_uri('https://dagshub.com/fabiencappelli/Projet_07.mlflow')\n",
    "\n",
    "font_path = os.path.expanduser(\"/content/drive/MyDrive/Colab Notebooks/fonts/Exo2-VariableFont_wght.ttf\")  # Remplacez par le chemin exact\n",
    "fm.fontManager.addfont(font_path)\n",
    "\n",
    "# Définir la police globale avec le nom de la police\n",
    "rcParams[\"font.family\"] = \"Exo 2\"\n",
    "# deux couleurs pertinentes pour aller avec la présentation\n",
    "bleuclair = (0.15, 0.55, 0.82)\n",
    "couleur_complementaire = (1 - bleuclair[0], 1 - bleuclair[1], 1 - bleuclair[2])\n",
    "bleufonce = \"#073642\"\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747738634164,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "ARPNqkM_1Z3t"
   },
   "outputs": [],
   "source": [
    "SEED = 34\n",
    "csvPath = '/content/drive/MyDrive/Colab Notebooks/Projet_07/df_cleaned.csv'\n",
    "imgPrezPath = '/content/drive/MyDrive/Colab Notebooks/Projet_07/presentationimg'\n",
    "checkpoint_path = '/content/drive/MyDrive/Colab Notebooks/Projet_07/outputs/checkpoints_2/.weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4801,
     "status": "ok",
     "timestamp": 1747738638966,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "WEBnN8hTyBFi"
   },
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv(csvPath, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxAIzYF2dlRj"
   },
   "source": [
    "# Fonctions pour le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1747738638970,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "4LnMVy-60jgp"
   },
   "outputs": [],
   "source": [
    "def create_callbacks(\n",
    "    checkpoint_path,\n",
    "    patience_es=6,\n",
    "    min_delta_es=0.01,\n",
    "    monitor_es='val_loss',\n",
    "    mode_es='min',\n",
    "    monitor_mc='val_accuracy',\n",
    "    mode_mc='max',\n",
    "    factor_lr=0.1,\n",
    "    cooldown_lr=5,\n",
    "    patience_lr=5,\n",
    "    min_lr=1e-5,\n",
    "    monitor_lr='val_loss',\n",
    "    mode_lr='min'\n",
    "):\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=patience_es,\n",
    "        min_delta=min_delta_es,\n",
    "        monitor=monitor_es,\n",
    "        mode=mode_es,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model_autosave = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        monitor=monitor_mc,\n",
    "        mode=mode_mc,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    lr_reducer = ReduceLROnPlateau(\n",
    "        factor=factor_lr,\n",
    "        cooldown=cooldown_lr,\n",
    "        patience=patience_lr,\n",
    "        min_lr=min_lr,\n",
    "        monitor=monitor_lr,\n",
    "        mode=mode_lr,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return [early_stopping, model_autosave, lr_reducer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1747738638979,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "oItVo1yS0ncX"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, lstm_units=64, learning_rate=0.001):\n",
    "    # https://keras.io/examples/nlp/bidirectional_lstm_imdb/\n",
    "    # Input for variable-length sequences of integers\n",
    "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    x = layers.Embedding(vocab_size, 256)(inputs)\n",
    "    # Add 2 bidirectional LSTMs\n",
    "    x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(lstm_units))(x)\n",
    "    predictions = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs, predictions)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1747738638982,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "iDIgCOcB0pqH"
   },
   "outputs": [],
   "source": [
    "def get_inference_time(model, X, n_runs=10):\n",
    "    _ = model.predict(X[:2], verbose=0)  # Pré-chauffage\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        _ = model.predict(X, verbose=0)\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "    mean_time = np.mean(times)\n",
    "    ms_per_sample = (mean_time / X.shape[0]) * 1000\n",
    "    return ms_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1747738638985,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "9ie3HOH1nU1t"
   },
   "outputs": [],
   "source": [
    "def train_pipeline(data, labels, num_words=10000, param_grid=None, random_state=34, sample_frac=None):\n",
    "    \"\"\"\n",
    "    data, labels: pd.Series ou array-like\n",
    "    param_grid: dict, paramètres pour la grid search\n",
    "    sample_frac: float (ex: 0.2 pour 20%), si None, tout le dataset est utilisé pour la grid search\n",
    "    \"\"\"\n",
    "\n",
    "    if sample_frac is not None:\n",
    "        data_sample, _, labels_sample, _ = train_test_split(\n",
    "          data, labels,\n",
    "          train_size=sample_frac,\n",
    "          random_state=random_state,\n",
    "          stratify=labels\n",
    "          )\n",
    "    else:\n",
    "        data_sample, labels_sample = data, labels\n",
    "\n",
    "    # Split train/val/test sur l'échantillon (pour la grid search)\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "        data_sample, labels_sample, test_size=0.15, random_state=random_state, stratify=labels_sample)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_trainval, y_trainval, test_size=0.1765, random_state=random_state, stratify=y_trainval)\n",
    "    # car O.85*0.1765~0.15\n",
    "\n",
    "    # Tokenizer (fit sur train seulement)\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    maxlen = 100\n",
    "    def encode(X): return pad_sequences(tokenizer.texts_to_sequences(X), maxlen=maxlen, padding='post')\n",
    "    X_train_enc = encode(X_train)\n",
    "    X_val_enc = encode(X_val)\n",
    "    X_test_enc = encode(X_test)\n",
    "    vocab_size = min(len(tokenizer.word_index) + 1, num_words)\n",
    "    y_train_arr = np.asarray(y_train).astype('float32')\n",
    "    y_val_arr = np.asarray(y_val).astype('float32')\n",
    "    y_test_arr = np.asarray(y_test).astype('float32')\n",
    "\n",
    "    # Paramètres de la grid search\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'lstm_units': [64, 128],\n",
    "            'batch_size': [64, 128],\n",
    "            'learning_rate': [0.001, 0.0005, 0.0001]\n",
    "          }\n",
    "    search = list(ParameterGrid(param_grid))\n",
    "\n",
    "    best_val_f1 = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    for params in search:\n",
    "        with mlflow.start_run(nested=True):\n",
    "            mlflow.log_params(params)\n",
    "            model = build_model(\n",
    "                vocab_size,\n",
    "                lstm_units=params['lstm_units'],\n",
    "                learning_rate=params['learning_rate']\n",
    "            )\n",
    "            callbacks = create_callbacks(checkpoint_path=checkpoint_path)\n",
    "            history = model.fit(\n",
    "                X_train_enc, y_train_arr,\n",
    "                validation_data=(X_val_enc, y_val_arr),\n",
    "                epochs=15,\n",
    "                batch_size=params['batch_size'],\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            val_pred_proba = model.predict(X_val_enc)\n",
    "            val_pred = (val_pred_proba > 0.5).astype(int)\n",
    "            val_f1 = f1_score(y_val_arr, val_pred)\n",
    "            val_acc = accuracy_score(y_val_arr, val_pred)\n",
    "            try:\n",
    "                val_roc_auc = roc_auc_score(y_val_arr, val_pred_proba)\n",
    "            except Exception:\n",
    "                val_roc_auc = np.nan\n",
    "            mlflow.log_metric(\"val_f1\", val_f1)\n",
    "            mlflow.log_metric(\"val_accuracy\", val_acc)\n",
    "            mlflow.log_metric(\"val_roc_auc\", val_roc_auc)\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                best_params = params\n",
    "                best_model = model\n",
    "\n",
    "    mlflow.log_params({\"best_\"+k: v for k, v in best_params.items()})\n",
    "\n",
    "    # Test set metrics\n",
    "    test_pred_proba = best_model.predict(X_test_enc)\n",
    "    test_pred = (test_pred_proba > 0.5).astype(int)\n",
    "    test_acc = accuracy_score(y_test_arr, test_pred)\n",
    "    test_f1 = f1_score(y_test_arr, test_pred)\n",
    "    try:\n",
    "        test_roc_auc = roc_auc_score(y_test_arr, test_pred_proba)\n",
    "    except Exception:\n",
    "        test_roc_auc = np.nan\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "    mlflow.log_metric(\"test_roc_auc\", test_roc_auc)\n",
    "\n",
    "    inf_time_ms = get_inference_time(best_model, X_test_enc, n_runs=10)\n",
    "    mlflow.log_metric(\"test_inference_time_ms_per_sample\", inf_time_ms)\n",
    "\n",
    "    mlflow.keras.log_model(best_model, \"model\")\n",
    "\n",
    "    print(f\"Best val_f1: {best_val_f1:.3f} | Test acc: {test_acc:.3f} | Test f1: {test_f1:.3f} | Test ROC AUC: {test_roc_auc:.3f} | Inf time (ms/sample): {inf_time_ms:.3f}\")\n",
    "    return best_model, best_params, best_val_f1, test_acc, test_f1, test_roc_auc, inf_time_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747738638996,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "Cps35eaboDyA"
   },
   "outputs": [],
   "source": [
    "def refit_best_model(data, labels, best_params, num_words=10000, random_state=34):\n",
    "    # Split complet (train/val/test sur tout le jeu de données)\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "        data, labels, test_size=0.15, random_state=random_state, stratify=labels)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_trainval, y_trainval, test_size=0.1765, random_state=random_state, stratify=y_trainval)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    maxlen = 100\n",
    "    def encode(X): return pad_sequences(tokenizer.texts_to_sequences(X), maxlen=maxlen, padding='post')\n",
    "    X_train_enc = encode(X_train)\n",
    "    X_val_enc = encode(X_val)\n",
    "    X_test_enc = encode(X_test)\n",
    "    vocab_size = min(len(tokenizer.word_index) + 1, num_words)\n",
    "    y_train_arr = np.asarray(y_train).astype('float32')\n",
    "    y_val_arr = np.asarray(y_val).astype('float32')\n",
    "    y_test_arr = np.asarray(y_test).astype('float32')\n",
    "\n",
    "    # Entraînement du modèle avec best_params\n",
    "    model = build_model(\n",
    "        vocab_size,\n",
    "        lstm_units=best_params['lstm_units'],\n",
    "        learning_rate=best_params['learning_rate']\n",
    "    )\n",
    "    callbacks = create_callbacks(checkpoint_path=\"final_model.weights.h5\")\n",
    "    history = model.fit(\n",
    "        X_train_enc, y_train_arr,\n",
    "        validation_data=(X_val_enc, y_val_arr),\n",
    "        epochs=15,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    # Log des métriques par époque :\n",
    "    hist = history.history\n",
    "    for metric_name, values in hist.items():\n",
    "        if metric_name == \"lr\":\n",
    "            continue\n",
    "        for epoch, value in enumerate(values):\n",
    "            mlflow.log_metric(metric_name, value, step=epoch+1)\n",
    "\n",
    "    # Évaluation sur test\n",
    "    test_pred_proba = model.predict(X_test_enc)\n",
    "    test_pred = (test_pred_proba > 0.5).astype(int)\n",
    "    test_acc = accuracy_score(y_test_arr, test_pred)\n",
    "    test_f1 = f1_score(y_test_arr, test_pred)\n",
    "    try:\n",
    "        test_roc_auc = roc_auc_score(y_test_arr, test_pred_proba)\n",
    "    except Exception:\n",
    "        test_roc_auc = np.nan\n",
    "    inf_time_ms = get_inference_time(model, X_test_enc, n_runs=10)\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "    mlflow.log_metric(\"test_roc_auc\", test_roc_auc)\n",
    "\n",
    "    inf_time_ms = get_inference_time(model, X_test_enc, n_runs=10)\n",
    "    mlflow.log_metric(\"test_inference_time_ms_per_sample\", inf_time_ms)\n",
    "\n",
    "    mlflow.keras.log_model(model, \"model\")\n",
    "\n",
    "    print(f\"Final test acc: {test_acc:.3f}, test f1: {test_f1:.3f}, test ROC AUC: {test_roc_auc:.3f}, inf. time (ms/sample): {inf_time_ms:.3f}\")\n",
    "    return model, history, test_acc, test_f1, test_roc_auc, inf_time_ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747738638999,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "uhqYBFtyM9yc"
   },
   "outputs": [],
   "source": [
    "def graphhistory(history, test_acc, filename):\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  hist = history.history\n",
    "  epochs = range(1, len(hist['loss']) + 1)\n",
    "  # Loss\n",
    "  plt.plot(epochs, hist['loss'],\n",
    "          label='Training loss',\n",
    "          linestyle='-',\n",
    "          linewidth=2,\n",
    "          color=bleufonce)\n",
    "  plt.plot(epochs, hist['val_loss'],\n",
    "          label='Validation loss',\n",
    "          linestyle='--',\n",
    "          linewidth=2,\n",
    "          color=bleufonce)\n",
    "\n",
    "  # Accuracy\n",
    "  plt.plot(epochs, hist['accuracy'],\n",
    "          label='Training accuracy',\n",
    "          linestyle='-',\n",
    "          linewidth=2,\n",
    "          color=bleuclair)\n",
    "  plt.plot(epochs, hist['val_accuracy'],\n",
    "          label='Validation accuracy',\n",
    "          linestyle='--',\n",
    "          linewidth=2,\n",
    "          color=couleur_complementaire)\n",
    "\n",
    "  # Ajout de l’accuracy test en ligne horizontale\n",
    "  plt.axhline(test_acc, linestyle=':', linewidth=2, color='darkgreen', label='Test accuracy')\n",
    "\n",
    "  # Mise en forme\n",
    "  plt.title(\"Évolution de la perte et de l'exactitude\", fontsize=14)\n",
    "  plt.xlabel(\"Époque\", fontsize=12)\n",
    "  plt.ylabel(\"Valeur\", fontsize=12)\n",
    "  plt.legend()\n",
    "  plt.grid(alpha=0.3)\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(os.path.join(imgPrezPath, filename + \".svg\"), format=\"svg\", bbox_inches=\"tight\", pad_inches=0.1)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxz-6IBg09ae"
   },
   "source": [
    "https://realpython.com/python-keras-text-classification/#choosing-a-data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ompDD-Gf4EjT"
   },
   "source": [
    "# Données brutes\n",
    "\n",
    "On repart sur les données brutes vu qu'elles nous ont donné le meilleur résultat précédemment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1747738639002,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "N9NFQseK4Wk1"
   },
   "outputs": [],
   "source": [
    "# Division en jeu d'entraînement et de test\n",
    "X = df_cleaned['text']\n",
    "y = df_cleaned['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1747738639179,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "ltjwCVg55KGC",
    "outputId": "0ee1ae30-956c-4b0d-c6a1-7c14268ee92e"
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"BLSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747738639181,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "TsZSTEPW5TGu"
   },
   "outputs": [],
   "source": [
    "callbacks = create_callbacks(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8948338,
     "status": "ok",
     "timestamp": 1747747587517,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "zEVrJwx05o9P",
    "outputId": "d5f69712-48f1-485e-aa08-1ceefe5ccc34"
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"GridSearch_sample_20pct\"):\n",
    "    best_model, best_params, best_val_f1, test_acc, test_f1, test_roc_auc, inf_time_ms = train_pipeline(X, y, num_words=10000, sample_frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5687207,
     "status": "ok",
     "timestamp": 1747753274727,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "aTLVUz-eougI",
    "outputId": "6dbeae8f-6ce9-4184-ced9-f1fb30e558bc"
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Final_refit_full_data\"):\n",
    "    model, history, test_acc, test_f1, test_roc_auc, inf_time_ms = refit_best_model(X, y, best_params=best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 2463,
     "status": "ok",
     "timestamp": 1747753681355,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "8HLfGFlZ6NxD",
    "outputId": "f485bce1-9f2c-4190-fbe5-01b8f1229173"
   },
   "outputs": [],
   "source": [
    "graphhistory(history, test_acc, \"BLSTMRefitHistory\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNg87Tso91SZHSdie7/aFt3",
   "gpuType": "T4",
   "mount_file_id": "1Ljs2aFYDhZ62lM78iMGZWiaLksHPTPUE",
   "provenance": [
    {
     "file_id": "1x0yBhA60AGTXDHRu_UIJQ6vZdZ4wlE69",
     "timestamp": 1747645836811
    },
    {
     "file_id": "1CPaV9L9kn1OJG8qebflnwgHXSfOY_WNo",
     "timestamp": 1745051496820
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
