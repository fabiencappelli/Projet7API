{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5drKlSgxUS5"
   },
   "source": [
    "# Imports et Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 30136,
     "status": "ok",
     "timestamp": 1747732576357,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "xPobTGhKv5qo",
    "outputId": "12dfa3c3-d8d4-427f-9f92-b6f7e212835d"
   },
   "outputs": [],
   "source": [
    "!pip install mlflow dagshub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 8859,
     "status": "ok",
     "timestamp": 1747732809169,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "gX0vLl_Txm7n",
    "outputId": "ca6e22e4-82bc-43b9-8c9e-2ba3c2c0d23d"
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import dagshub\n",
    "import mlflow.keras\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import time\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Récupère automatiquement le secret\n",
    "dagshub_token = userdata.get('DAGSHUB_TOKEN')\n",
    "\n",
    "# Initialisation Dagshub\n",
    "dagshub.auth.add_app_token(dagshub_token)\n",
    "\n",
    "# Connecter MLflow à Dagshub\n",
    "dagshub.init(repo_owner='fabiencappelli', repo_name='Projet_07', mlflow=True)\n",
    "\n",
    "# Configure MLflow pour pointer vers Dagshub\n",
    "mlflow.set_tracking_uri('https://dagshub.com/fabiencappelli/Projet_07.mlflow')\n",
    "\n",
    "font_path = os.path.expanduser(\"/content/drive/MyDrive/Colab Notebooks/fonts/Exo2-VariableFont_wght.ttf\")  # Remplacez par le chemin exact\n",
    "fm.fontManager.addfont(font_path)\n",
    "\n",
    "# Définir la police globale avec le nom de la police\n",
    "rcParams[\"font.family\"] = \"Exo 2\"\n",
    "# deux couleurs pertinentes pour aller avec la présentation\n",
    "bleuclair = (0.15, 0.55, 0.82)\n",
    "couleur_complementaire = (1 - bleuclair[0], 1 - bleuclair[1], 1 - bleuclair[2])\n",
    "bleufonce = \"#073642\"\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747732823793,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "ARPNqkM_1Z3t"
   },
   "outputs": [],
   "source": [
    "SEED = 34\n",
    "csvPath = '/content/drive/MyDrive/Colab Notebooks/Projet_07/df_cleaned.csv'\n",
    "imgPrezPath = '/content/drive/MyDrive/Colab Notebooks/Projet_07/presentationimg'\n",
    "checkpoint_path = '/content/drive/MyDrive/Colab Notebooks/Projet_07/outputs/checkpoints_2/.weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2748,
     "status": "ok",
     "timestamp": 1747732831670,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "WEBnN8hTyBFi"
   },
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv(csvPath, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxAIzYF2dlRj"
   },
   "source": [
    "# Fonctions pour le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747732831687,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "4LnMVy-60jgp"
   },
   "outputs": [],
   "source": [
    "def create_callbacks(\n",
    "    checkpoint_path,\n",
    "    patience_es=6,\n",
    "    min_delta_es=0.01,\n",
    "    monitor_es='val_loss',\n",
    "    mode_es='min',\n",
    "    monitor_mc='val_accuracy',\n",
    "    mode_mc='max',\n",
    "    factor_lr=0.1,\n",
    "    cooldown_lr=5,\n",
    "    patience_lr=5,\n",
    "    min_lr=1e-5,\n",
    "    monitor_lr='val_loss',\n",
    "    mode_lr='min'\n",
    "):\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=patience_es,\n",
    "        min_delta=min_delta_es,\n",
    "        monitor=monitor_es,\n",
    "        mode=mode_es,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model_autosave = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        monitor=monitor_mc,\n",
    "        mode=mode_mc,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    lr_reducer = ReduceLROnPlateau(\n",
    "        factor=factor_lr,\n",
    "        cooldown=cooldown_lr,\n",
    "        patience=patience_lr,\n",
    "        min_lr=min_lr,\n",
    "        monitor=monitor_lr,\n",
    "        mode=mode_lr,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return [early_stopping, model_autosave, lr_reducer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747732834441,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "oItVo1yS0ncX"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_matrix, embedding_dim, maxlen=100, num_filters=64, kernel_size=5, dropout=0.5, learning_rate=0.001):\n",
    "    inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    x = layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=maxlen,\n",
    "        trainable=False\n",
    "    )(inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(num_filters, kernel_size, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "    x = layers.Conv1D(num_filters, kernel_size, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(num_filters, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, predictions)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747732837109,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "h1cXSoJw-JuQ"
   },
   "outputs": [],
   "source": [
    "def get_inference_time(model, X, n_runs=10):\n",
    "    _ = model.predict(X[:2], verbose=0)  # Pré-chauffage\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        _ = model.predict(X, verbose=0)\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "    mean_time = np.mean(times)\n",
    "    ms_per_sample = (mean_time / X.shape[0]) * 1000\n",
    "    return ms_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747732838445,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "sbT5UVSV088P"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim, num_words=10000):\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            # Modification du tuto pour économiser, ne pas embed plus que nécessaire\n",
    "            idx = word_index.get(word)\n",
    "            # On ne garde que les index < num_words\n",
    "            if idx is not None and idx < num_words:\n",
    "                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1747732841114,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "iDIgCOcB0pqH"
   },
   "outputs": [],
   "source": [
    "def train_pipeline(data,\n",
    "                   labels,\n",
    "                   embedding_dim,\n",
    "                   filepath,\n",
    "                   num_words=10000,\n",
    "                   param_grid=None,\n",
    "                   random_state=SEED,\n",
    "                   sample_frac=None):\n",
    "    \"\"\"\n",
    "    data, labels: pd.Series ou array-like\n",
    "    param_grid: dict, paramètres pour la grid search\n",
    "    sample_frac: float (ex: 0.2 pour 20%), si None, tout le dataset est utilisé pour la grid search\n",
    "    \"\"\"\n",
    "\n",
    "    if sample_frac is not None:\n",
    "        data_sample, _, labels_sample, _ = train_test_split(\n",
    "          data, labels,\n",
    "          train_size=sample_frac,\n",
    "          random_state=random_state,\n",
    "          stratify=labels\n",
    "          )\n",
    "    else:\n",
    "        data_sample, labels_sample = data, labels\n",
    "\n",
    "    # Split train/val/test sur l'échantillon (pour la grid search)\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "        data_sample, labels_sample, test_size=0.15, random_state=random_state, stratify=labels_sample)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_trainval, y_trainval, test_size=0.1765, random_state=random_state, stratify=y_trainval)\n",
    "    # car O.85*0.1765~0.15\n",
    "\n",
    "    # Tokenizer (fit sur train seulement)\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    maxlen = 100\n",
    "    def encode(X): return pad_sequences(tokenizer.texts_to_sequences(X), maxlen=maxlen, padding='post')\n",
    "    X_train_enc = encode(X_train)\n",
    "    X_val_enc = encode(X_val)\n",
    "    X_test_enc = encode(X_test)\n",
    "    dictionary = tokenizer.word_index\n",
    "    vocab_size = num_words\n",
    "    y_train_arr = np.asarray(y_train).astype('float32')\n",
    "    y_val_arr = np.asarray(y_val).astype('float32')\n",
    "    y_test_arr = np.asarray(y_test).astype('float32')\n",
    "\n",
    "    # Embedding\n",
    "    embedding_matrix = create_embedding_matrix(filepath, dictionary, embedding_dim, num_words)\n",
    "\n",
    "    # Paramètres de la grid search\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'num_filters': [64],\n",
    "            'kernel_size': [5, 7],\n",
    "            'dropout': [0.5, 0.3],\n",
    "            'batch_size': [64, 128],\n",
    "            'learning_rate': [0.001, 0.0005, 0.0001]\n",
    "            }\n",
    "    search = list(ParameterGrid(param_grid))\n",
    "\n",
    "    best_val_f1 = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    for params in search:\n",
    "        with mlflow.start_run(nested=True):\n",
    "            mlflow.log_params(params)\n",
    "            model = build_model(\n",
    "                vocab_size,\n",
    "                embedding_matrix,\n",
    "                embedding_dim,\n",
    "                num_filters=params['num_filters'],\n",
    "                kernel_size=params['kernel_size'],\n",
    "                dropout=params['dropout'],\n",
    "                learning_rate=params['learning_rate']\n",
    "            )\n",
    "            callbacks = create_callbacks(checkpoint_path=checkpoint_path)\n",
    "            history = model.fit(\n",
    "                X_train_enc, y_train_arr,\n",
    "                validation_data=(X_val_enc, y_val_arr),\n",
    "                epochs=15,\n",
    "                batch_size=params['batch_size'],\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            val_pred_proba = model.predict(X_val_enc)\n",
    "            val_pred = (val_pred_proba > 0.5).astype(int)\n",
    "            val_f1 = f1_score(y_val_arr, val_pred)\n",
    "            val_acc = accuracy_score(y_val_arr, val_pred)\n",
    "            try:\n",
    "                val_roc_auc = roc_auc_score(y_val_arr, val_pred_proba)\n",
    "            except Exception:\n",
    "                val_roc_auc = np.nan\n",
    "            mlflow.log_metric(\"val_f1\", val_f1)\n",
    "            mlflow.log_metric(\"val_accuracy\", val_acc)\n",
    "            mlflow.log_metric(\"val_roc_auc\", val_roc_auc)\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                best_params = params\n",
    "                best_model = model\n",
    "\n",
    "    mlflow.log_params({\"best_\"+k: v for k, v in best_params.items()})\n",
    "\n",
    "    # Test set metrics\n",
    "    test_pred_proba = best_model.predict(X_test_enc)\n",
    "    test_pred = (test_pred_proba > 0.5).astype(int)\n",
    "    test_acc = accuracy_score(y_test_arr, test_pred)\n",
    "    test_f1 = f1_score(y_test_arr, test_pred)\n",
    "    try:\n",
    "        test_roc_auc = roc_auc_score(y_test_arr, test_pred_proba)\n",
    "    except Exception:\n",
    "        test_roc_auc = np.nan\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "    mlflow.log_metric(\"test_roc_auc\", test_roc_auc)\n",
    "\n",
    "    inf_time_ms = get_inference_time(best_model, X_test_enc, n_runs=10)\n",
    "    mlflow.log_metric(\"test_inference_time_ms_per_sample\", inf_time_ms)\n",
    "\n",
    "    mlflow.keras.log_model(best_model, \"model\")\n",
    "\n",
    "    print(f\"Best val_f1: {best_val_f1:.3f} | Test acc: {test_acc:.3f} | Test f1: {test_f1:.3f} | Test ROC AUC: {test_roc_auc:.3f} | Inf time (ms/sample): {inf_time_ms:.3f}\")\n",
    "    return best_model, best_params, best_val_f1, test_acc, test_f1, test_roc_auc, inf_time_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxz-6IBg09ae"
   },
   "source": [
    "https://realpython.com/python-keras-text-classification/#choosing-a-data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1747732845342,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "41rWlTKeFxjT"
   },
   "outputs": [],
   "source": [
    "def refit_best_model(data,\n",
    "                     labels,\n",
    "                     embedding_dim,\n",
    "                     filepath,\n",
    "                     best_params,\n",
    "                     num_words=10000,\n",
    "                     random_state=SEED\n",
    "                     ):\n",
    "    # Split complet (train/val/test sur tout le jeu de données)\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "        data, labels, test_size=0.15, random_state=random_state, stratify=labels)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_trainval, y_trainval, test_size=0.1765, random_state=random_state, stratify=y_trainval)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    maxlen = 100\n",
    "    def encode(X): return pad_sequences(tokenizer.texts_to_sequences(X), maxlen=maxlen, padding='post')\n",
    "    X_train_enc = encode(X_train)\n",
    "    X_val_enc = encode(X_val)\n",
    "    X_test_enc = encode(X_test)\n",
    "    dictionary = tokenizer.word_index\n",
    "    vocab_size = num_words\n",
    "    y_train_arr = np.asarray(y_train).astype('float32')\n",
    "    y_val_arr = np.asarray(y_val).astype('float32')\n",
    "    y_test_arr = np.asarray(y_test).astype('float32')\n",
    "\n",
    "    # Embedding\n",
    "    embedding_matrix = create_embedding_matrix(filepath,dictionary, embedding_dim, num_words)\n",
    "\n",
    "\n",
    "    # Entraînement du modèle avec best_params\n",
    "    model = build_model(\n",
    "        vocab_size,\n",
    "        embedding_matrix,\n",
    "        embedding_dim,\n",
    "        num_filters=best_params['num_filters'],\n",
    "        kernel_size=best_params['kernel_size'],\n",
    "        dropout=best_params['dropout'],\n",
    "        learning_rate=best_params['learning_rate']\n",
    "        )\n",
    "    callbacks = create_callbacks(checkpoint_path=\"final_model.weights.h5\")\n",
    "    history = model.fit(\n",
    "        X_train_enc, y_train_arr,\n",
    "        validation_data=(X_val_enc, y_val_arr),\n",
    "        epochs=15,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    # Log des métriques par époque :\n",
    "    hist = history.history\n",
    "    for metric_name, values in hist.items():\n",
    "        if metric_name == \"lr\":\n",
    "            continue\n",
    "        for epoch, value in enumerate(values):\n",
    "            mlflow.log_metric(metric_name, value, step=epoch+1)\n",
    "\n",
    "    # Évaluation sur test\n",
    "    test_pred_proba = model.predict(X_test_enc)\n",
    "    test_pred = (test_pred_proba > 0.5).astype(int)\n",
    "    test_acc = accuracy_score(y_test_arr, test_pred)\n",
    "    test_f1 = f1_score(y_test_arr, test_pred)\n",
    "    try:\n",
    "        test_roc_auc = roc_auc_score(y_test_arr, test_pred_proba)\n",
    "    except Exception:\n",
    "        test_roc_auc = np.nan\n",
    "    inf_time_ms = get_inference_time(model, X_test_enc, n_runs=10)\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "    mlflow.log_metric(\"test_roc_auc\", test_roc_auc)\n",
    "\n",
    "    inf_time_ms = get_inference_time(model, X_test_enc, n_runs=10)\n",
    "    mlflow.log_metric(\"test_inference_time_ms_per_sample\", inf_time_ms)\n",
    "\n",
    "    mlflow.keras.log_model(model, \"model\")\n",
    "\n",
    "    print(f\"Final test acc: {test_acc:.3f}, test f1: {test_f1:.3f}, test ROC AUC: {test_roc_auc:.3f}, inf. time (ms/sample): {inf_time_ms:.3f}\")\n",
    "    return model, history, test_acc, test_f1, test_roc_auc, inf_time_ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1747732848710,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "U1HduODhtPaz"
   },
   "outputs": [],
   "source": [
    "def graphhistory(history, test_acc, filename):\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  hist = history.history\n",
    "  epochs = range(1, len(hist['loss']) + 1)\n",
    "  # Loss\n",
    "  plt.plot(epochs, hist['loss'],\n",
    "          label='Training loss',\n",
    "          linestyle='-',\n",
    "          linewidth=2,\n",
    "          color=bleufonce)\n",
    "  plt.plot(epochs, hist['val_loss'],\n",
    "          label='Validation loss',\n",
    "          linestyle='--',\n",
    "          linewidth=2,\n",
    "          color=bleufonce)\n",
    "\n",
    "  # Accuracy\n",
    "  plt.plot(epochs, hist['accuracy'],\n",
    "          label='Training accuracy',\n",
    "          linestyle='-',\n",
    "          linewidth=2,\n",
    "          color=bleuclair)\n",
    "  plt.plot(epochs, hist['val_accuracy'],\n",
    "          label='Validation accuracy',\n",
    "          linestyle='--',\n",
    "          linewidth=2,\n",
    "          color=couleur_complementaire)\n",
    "\n",
    "  # Ajout de l’accuracy test en ligne horizontale\n",
    "  plt.axhline(test_acc, linestyle=':', linewidth=2, color='darkgreen', label='Test accuracy')\n",
    "\n",
    "  # Mise en forme\n",
    "  plt.title(\"Évolution de la perte et de l'exactitude\", fontsize=14)\n",
    "  plt.xlabel(\"Époque\", fontsize=12)\n",
    "  plt.ylabel(\"Valeur\", fontsize=12)\n",
    "  plt.legend()\n",
    "  plt.grid(alpha=0.3)\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(os.path.join(imgPrezPath, filename + \".svg\"), format=\"svg\", bbox_inches=\"tight\", pad_inches=0.1)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ompDD-Gf4EjT"
   },
   "source": [
    "# Données brutes\n",
    "\n",
    "On repart sur les données brutes vu qu'elles nous ont donné le meilleur résultat précédemment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747732851841,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "N9NFQseK4Wk1"
   },
   "outputs": [],
   "source": [
    "X = df_cleaned['text']\n",
    "y = df_cleaned['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SbmQc-z5hcQ"
   },
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97,
     "status": "ok",
     "timestamp": 1747692425456,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "ltjwCVg55KGC",
    "outputId": "07c40651-04ce-40ad-8539-bfb5c9517d30"
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Embedding_Glove.twitter.27B.200d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5rFxGiQ1CaF"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "filepath = '/content/drive/MyDrive/Colab Notebooks/Projet_07/glove.twitter.27B/glove.twitter.27B.200d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4128312,
     "status": "ok",
     "timestamp": 1747696553775,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "ULRbxTNDHQcw",
    "outputId": "da7ccd09-bd70-40ad-82c4-5462450716af"
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"GridSearch_sample_20pct\"):\n",
    "  best_model, best_params, best_val_f1, test_acc, test_f1, test_roc_auc, inf_time_ms = train_pipeline(X, y, embedding_dim, filepath, sample_frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1374701,
     "status": "ok",
     "timestamp": 1747697928480,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "xSI46p4lJy2N",
    "outputId": "b5112e0f-39c6-40d3-9b68-91b27fcef3d0"
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Final_refit_full_data\"):\n",
    "    model, history, test_acc, test_f1, test_roc_auc, inf_time_ms = refit_best_model(X, y, embedding_dim, filepath, best_params=best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 1316,
     "status": "ok",
     "timestamp": 1747697929848,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "tsxHdas6J8lU",
    "outputId": "feeac658-f9af-421c-b583-ec3d7764b38d"
   },
   "outputs": [],
   "source": [
    "graphhistory(history, test_acc, \"GloveRefitHistory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE9Yk9s0usMq"
   },
   "source": [
    "# Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 883
    },
    "executionInfo": {
     "elapsed": 14780,
     "status": "ok",
     "timestamp": 1747732688857,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "tIQLUdhx2kOL",
    "outputId": "234dedfc-f7e7-4399-ecc1-c6f5569b85dd"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall --no-cache-dir gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 92263,
     "status": "ok",
     "timestamp": 1747732791597,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "rSayBtr5tZjp"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# chargement (binaire)\n",
    "w2v_path = '/content/drive/MyDrive/Colab Notebooks/Projet_07/GoogleNews-vectors-negative300.bin'\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1747732863368,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "JMy3nOsSuCur"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(w2v, word_index, embedding_dim=300, num_words=10000):\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    for word, idx in word_index.items():\n",
    "        if idx is not None and idx < num_words:\n",
    "            if word in w2v:\n",
    "                embedding_matrix[idx] = w2v[word]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1747732864987,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "wST0ysDJuRn9"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1383,
     "status": "ok",
     "timestamp": 1747732869189,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "Xt_pHsl9uVap",
    "outputId": "787262bd-3bfa-4d6d-db55-2063a1549e23"
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Embedding_GoogleNews-vectors-negative300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4121961,
     "status": "ok",
     "timestamp": 1747736995825,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "qy9_zlvtvBJA",
    "outputId": "45ec8536-a002-4217-cb3b-802189cee7f7"
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"GridSearch_sample_20pct\"):\n",
    "  best_model_2, best_params_2, best_val_f1_2, test_acc_2, test_f1_2, test_roc_auc_2, inf_time_ms_2 = train_pipeline(X, y, embedding_dim, w2v, sample_frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 896641,
     "status": "ok",
     "timestamp": 1747737892573,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "Jc8nVdFs-mAD",
    "outputId": "fd49138f-3696-49a5-d6ff-28cded07100b"
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Final_refit_full_data\"):\n",
    "    model_2, history_2, test_acc_2, test_f1_2, test_roc_auc_2, inf_time_ms_2 = refit_best_model(X, y, embedding_dim, w2v, best_params=best_params_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 2461,
     "status": "ok",
     "timestamp": 1747737895037,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "qbLiQDk1umRj",
    "outputId": "666361e5-a074-4339-cfe4-beaa3bc0bb33"
   },
   "outputs": [],
   "source": [
    "graphhistory(history_2, test_acc_2, \"GoogleRefitHistory\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNZGmhwtkfRo1oU+4qbvM8o",
   "gpuType": "T4",
   "mount_file_id": "1AjcOLyhXjq0NDwAzZRK5cHJ-t7uPB3q7",
   "provenance": [
    {
     "file_id": "1CPaV9L9kn1OJG8qebflnwgHXSfOY_WNo",
     "timestamp": 1747480197011
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
