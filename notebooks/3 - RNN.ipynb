{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDhQHAfFc_fS"
   },
   "source": [
    "# Installs et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 32300,
     "status": "ok",
     "timestamp": 1747653348261,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "G__x167acvS_",
    "outputId": "726d92b1-a580-4617-9115-f30896935e5e"
   },
   "outputs": [],
   "source": [
    "!pip install mlflow dagshub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5403,
     "status": "ok",
     "timestamp": 1747653372698,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "5Lz5qDOode1o",
    "outputId": "669dae90-a0f9-4559-fafe-973e4e2396d4"
   },
   "outputs": [],
   "source": [
    "!pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747653372704,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "uN69FSYvVC9u"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13227,
     "status": "ok",
     "timestamp": 1747653385934,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "YnVK6Wh1dEuj"
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import dagshub\n",
    "import mlflow.keras\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import time\n",
    "from google.colab import userdata\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "executionInfo": {
     "elapsed": 1118,
     "status": "ok",
     "timestamp": 1747653387015,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "x40rHjWndGVb",
    "outputId": "0a72a4b6-aa3a-41ba-d43f-d58b2f44ab81"
   },
   "outputs": [],
   "source": [
    "# Récupère automatiquement le secret\n",
    "dagshub_token = userdata.get('DAGSHUB_TOKEN')\n",
    "\n",
    "# Initialisation Dagshub\n",
    "dagshub.auth.add_app_token(dagshub_token)\n",
    "\n",
    "# Connecter MLflow à Dagshub\n",
    "dagshub.init(repo_owner='fabiencappelli', repo_name='Projet_07', mlflow=True)\n",
    "\n",
    "# Configure MLflow pour pointer vers Dagshub\n",
    "mlflow.set_tracking_uri('https://dagshub.com/fabiencappelli/Projet_07.mlflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2971,
     "status": "ok",
     "timestamp": 1747653389986,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "uNZm1gu1dISW"
   },
   "outputs": [],
   "source": [
    "font_path = os.path.expanduser(\"/content/drive/MyDrive/Colab Notebooks/fonts/Exo2-VariableFont_wght.ttf\")  # Remplacez par le chemin exact\n",
    "fm.fontManager.addfont(font_path)\n",
    "\n",
    "# Définir la police globale avec le nom de la police\n",
    "rcParams[\"font.family\"] = \"Exo 2\"\n",
    "# deux couleurs pertinentes pour aller avec la présentation\n",
    "bleuclair = (0.15, 0.55, 0.82)\n",
    "couleur_complementaire = (1 - bleuclair[0], 1 - bleuclair[1], 1 - bleuclair[2])\n",
    "bleufonce = \"#073642\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1747653390063,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "ZboFFkW2dKhU"
   },
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 620,
     "status": "ok",
     "timestamp": 1747653390681,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "7Ao0M_w7dNn1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1747653390832,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "TVCg6kkpdQSs"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1747653390913,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "bIbjdC31dhZ7"
   },
   "outputs": [],
   "source": [
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11656,
     "status": "ok",
     "timestamp": 1747653402567,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "ZaE9He1RvPSN"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1747653402569,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "1jPMWw23NHM5"
   },
   "outputs": [],
   "source": [
    "imgPrezPath = '/content/drive/MyDrive/Colab Notebooks/Projet_07/presentationimg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747653402570,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "PDyGq9AmdSap"
   },
   "outputs": [],
   "source": [
    "csvPath = '/content/drive/MyDrive/Colab Notebooks/Projet_07/df_cleaned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6656,
     "status": "ok",
     "timestamp": 1747653409221,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "-qn5aQDMdUcI"
   },
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv(csvPath, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1747653409261,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "_SEJE1XjdWzi",
    "outputId": "698f0092-8d12-4240-bf0b-33ec1b0c125f"
   },
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxAIzYF2dlRj"
   },
   "source": [
    "# Fonctions pour le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1747653409266,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "C3FL3LCdd3if"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = '/content/drive/MyDrive/Colab Notebooks/Projet_07/outputs/checkpoints/.weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1747653409294,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "ptcjoP2UkiKL"
   },
   "outputs": [],
   "source": [
    "def create_callbacks(\n",
    "    checkpoint_path,\n",
    "    patience_es=6,\n",
    "    min_delta_es=0.01,\n",
    "    monitor_es='val_loss',\n",
    "    mode_es='min',\n",
    "    monitor_mc='val_accuracy',\n",
    "    mode_mc='max',\n",
    "    factor_lr=0.1,\n",
    "    cooldown_lr=5,\n",
    "    patience_lr=5,\n",
    "    min_lr=1e-5,\n",
    "    monitor_lr='val_loss',\n",
    "    mode_lr='min'\n",
    "):\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=patience_es,\n",
    "        min_delta=min_delta_es,\n",
    "        monitor=monitor_es,\n",
    "        mode=mode_es,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model_autosave = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        monitor=monitor_mc,\n",
    "        mode=mode_mc,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    lr_reducer = ReduceLROnPlateau(\n",
    "        factor=factor_lr,\n",
    "        cooldown=cooldown_lr,\n",
    "        patience=patience_lr,\n",
    "        min_lr=min_lr,\n",
    "        monitor=monitor_lr,\n",
    "        mode=mode_lr,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return [early_stopping, model_autosave, lr_reducer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747653409296,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "Xj13PKE_lOfQ"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim=128, num_filters=64, kernel_size=5, dropout=0.5, learning_rate=0.001):\n",
    "    inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    x = layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(num_filters, kernel_size, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "    x = layers.Conv1D(num_filters, kernel_size, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(num_filters, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, predictions)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1747653409297,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "Wv0ozz6aBSyl"
   },
   "outputs": [],
   "source": [
    "def get_inference_time(model, X, n_runs=10):\n",
    "    _ = model.predict(X[:2], verbose=0)  # Pré-chauffage\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        _ = model.predict(X, verbose=0)\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "    mean_time = np.mean(times)\n",
    "    ms_per_sample = (mean_time / X.shape[0]) * 1000\n",
    "    return ms_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1747653409299,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "E-HmZTD8BvVM"
   },
   "outputs": [],
   "source": [
    "def train_pipeline(data, labels, num_words, param_grid=None, random_state=34, sample_frac=None):\n",
    "    \"\"\"\n",
    "    data, labels: pd.Series ou array-like\n",
    "    param_grid: dict, paramètres pour la grid search\n",
    "    sample_frac: float (ex: 0.2 pour 20%), si None, tout le dataset est utilisé pour la grid search\n",
    "    \"\"\"\n",
    "\n",
    "    if sample_frac is not None:\n",
    "        data_sample, _, labels_sample, _ = train_test_split(\n",
    "          data, labels,\n",
    "          train_size=sample_frac,\n",
    "          random_state=random_state,\n",
    "          stratify=labels\n",
    "          )\n",
    "    else:\n",
    "        data_sample, labels_sample = data, labels\n",
    "\n",
    "    # Split train/val/test sur l'échantillon (pour la grid search)\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "        data_sample, labels_sample, test_size=0.15, random_state=random_state, stratify=labels_sample)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_trainval, y_trainval, test_size=0.1765, random_state=random_state, stratify=y_trainval)\n",
    "    # car O.85*0.1765~0.15\n",
    "\n",
    "    # Tokenizer (fit sur train seulement)\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    maxlen = 100\n",
    "    def encode(X): return pad_sequences(tokenizer.texts_to_sequences(X), maxlen=maxlen, padding='post')\n",
    "    X_train_enc = encode(X_train)\n",
    "    X_val_enc = encode(X_val)\n",
    "    X_test_enc = encode(X_test)\n",
    "    vocab_size = min(len(tokenizer.word_index) + 1, num_words)\n",
    "    y_train_arr = np.asarray(y_train).astype('float32')\n",
    "    y_val_arr = np.asarray(y_val).astype('float32')\n",
    "    y_test_arr = np.asarray(y_test).astype('float32')\n",
    "\n",
    "    # Paramètres de la grid search\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "        'embedding_dim': [128],\n",
    "        'num_filters': [64, 128],\n",
    "        'kernel_size': [5, 7],\n",
    "        'dropout': [0.5, 0.3],\n",
    "        'batch_size': [64, 128],\n",
    "        'learning_rate': [0.001, 0.0005, 0.0001]\n",
    "        }\n",
    "    search = list(ParameterGrid(param_grid))\n",
    "\n",
    "    best_val_f1 = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    for params in search:\n",
    "        with mlflow.start_run(nested=True):\n",
    "            mlflow.log_params(params)\n",
    "            model = build_model(\n",
    "                vocab_size,\n",
    "                embedding_dim=params['embedding_dim'],\n",
    "                num_filters=params['num_filters'],\n",
    "                kernel_size=params['kernel_size'],\n",
    "                dropout=params['dropout'],\n",
    "                learning_rate=params['learning_rate']\n",
    "            )\n",
    "            callbacks = create_callbacks(checkpoint_path=checkpoint_path)\n",
    "            history = model.fit(\n",
    "                X_train_enc, y_train_arr,\n",
    "                validation_data=(X_val_enc, y_val_arr),\n",
    "                epochs=15,\n",
    "                batch_size=params['batch_size'],\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            val_pred_proba = model.predict(X_val_enc)\n",
    "            val_pred = (val_pred_proba > 0.5).astype(int)\n",
    "            val_f1 = f1_score(y_val_arr, val_pred)\n",
    "            val_acc = accuracy_score(y_val_arr, val_pred)\n",
    "            try:\n",
    "                val_roc_auc = roc_auc_score(y_val_arr, val_pred_proba)\n",
    "            except Exception:\n",
    "                val_roc_auc = np.nan\n",
    "            mlflow.log_metric(\"val_f1\", val_f1)\n",
    "            mlflow.log_metric(\"val_accuracy\", val_acc)\n",
    "            mlflow.log_metric(\"val_roc_auc\", val_roc_auc)\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                best_params = params\n",
    "                best_model = model\n",
    "\n",
    "    mlflow.log_params({\"best_\"+k: v for k, v in best_params.items()})\n",
    "\n",
    "    # Test set metrics\n",
    "    test_pred_proba = best_model.predict(X_test_enc)\n",
    "    test_pred = (test_pred_proba > 0.5).astype(int)\n",
    "    test_acc = accuracy_score(y_test_arr, test_pred)\n",
    "    test_f1 = f1_score(y_test_arr, test_pred)\n",
    "    try:\n",
    "        test_roc_auc = roc_auc_score(y_test_arr, test_pred_proba)\n",
    "    except Exception:\n",
    "        test_roc_auc = np.nan\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "    mlflow.log_metric(\"test_roc_auc\", test_roc_auc)\n",
    "\n",
    "    inf_time_ms = get_inference_time(best_model, X_test_enc, n_runs=10)\n",
    "    mlflow.log_metric(\"test_inference_time_ms_per_sample\", inf_time_ms)\n",
    "\n",
    "    mlflow.keras.log_model(best_model, \"model\")\n",
    "\n",
    "    print(f\"Best val_f1: {best_val_f1:.3f} | Test acc: {test_acc:.3f} | Test f1: {test_f1:.3f} | Test ROC AUC: {test_roc_auc:.3f} | Inf time (ms/sample): {inf_time_ms:.3f}\")\n",
    "    return best_model, best_params, best_val_f1, test_acc, test_f1, test_roc_auc, inf_time_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747653409324,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "Bm02ZRqkO7IQ"
   },
   "outputs": [],
   "source": [
    "def refit_best_model(data, labels, num_words, best_params, random_state=34):\n",
    "    # Split complet (train/val/test sur tout le jeu de données)\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "        data, labels, test_size=0.15, random_state=random_state, stratify=labels)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_trainval, y_trainval, test_size=0.1765, random_state=random_state, stratify=y_trainval)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    maxlen = 100\n",
    "    def encode(X): return pad_sequences(tokenizer.texts_to_sequences(X), maxlen=maxlen, padding='post')\n",
    "    X_train_enc = encode(X_train)\n",
    "    X_val_enc = encode(X_val)\n",
    "    X_test_enc = encode(X_test)\n",
    "    vocab_size = min(len(tokenizer.word_index) + 1, num_words)\n",
    "    y_train_arr = np.asarray(y_train).astype('float32')\n",
    "    y_val_arr = np.asarray(y_val).astype('float32')\n",
    "    y_test_arr = np.asarray(y_test).astype('float32')\n",
    "\n",
    "    # Entraînement du modèle avec best_params\n",
    "    model = build_model(\n",
    "        vocab_size,\n",
    "        embedding_dim=best_params['embedding_dim'],\n",
    "        num_filters=best_params['num_filters'],\n",
    "        kernel_size=best_params['kernel_size'],\n",
    "        dropout=best_params['dropout'],\n",
    "        learning_rate=best_params['learning_rate']\n",
    "    )\n",
    "    callbacks = create_callbacks(checkpoint_path=\"final_model.weights.h5\")\n",
    "    history = model.fit(\n",
    "        X_train_enc, y_train_arr,\n",
    "        validation_data=(X_val_enc, y_val_arr),\n",
    "        epochs=15,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    # Log des métriques par époque :\n",
    "    hist = history.history\n",
    "    for metric_name, values in hist.items():\n",
    "        if metric_name == \"lr\":\n",
    "            continue\n",
    "        for epoch, value in enumerate(values):\n",
    "            mlflow.log_metric(metric_name, value, step=epoch+1)\n",
    "\n",
    "    # Évaluation sur test\n",
    "    test_pred_proba = model.predict(X_test_enc)\n",
    "    test_pred = (test_pred_proba > 0.5).astype(int)\n",
    "    test_acc = accuracy_score(y_test_arr, test_pred)\n",
    "    test_f1 = f1_score(y_test_arr, test_pred)\n",
    "    try:\n",
    "        test_roc_auc = roc_auc_score(y_test_arr, test_pred_proba)\n",
    "    except Exception:\n",
    "        test_roc_auc = np.nan\n",
    "    inf_time_ms = get_inference_time(model, X_test_enc, n_runs=10)\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "    mlflow.log_metric(\"test_roc_auc\", test_roc_auc)\n",
    "\n",
    "    inf_time_ms = get_inference_time(model, X_test_enc, n_runs=10)\n",
    "    mlflow.log_metric(\"test_inference_time_ms_per_sample\", inf_time_ms)\n",
    "\n",
    "    mlflow.keras.log_model(model, \"model\")\n",
    "\n",
    "    print(f\"Final test acc: {test_acc:.3f}, test f1: {test_f1:.3f}, test ROC AUC: {test_roc_auc:.3f}, inf. time (ms/sample): {inf_time_ms:.3f}\")\n",
    "    return model, history, test_acc, test_f1, test_roc_auc, inf_time_ms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GKY4LLYAnQg"
   },
   "source": [
    "# 1. Données brutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6isik02iSVTi"
   },
   "source": [
    "## Données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVBhuMXYH_xm"
   },
   "source": [
    "On va dans un premier lieu trouver le nombre de mots pertinent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24833,
     "status": "ok",
     "timestamp": 1747653434161,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "KX8ZcMtWRsKb",
    "outputId": "0cf2337a-42dc-4f68-a15a-749b37ac4d15"
   },
   "outputs": [],
   "source": [
    "X = df_cleaned['text']\n",
    "y = df_cleaned['target']\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "total_mots = len(tokenizer.word_index)\n",
    "print(\"Nombre total de mots uniques :\", total_mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 1740,
     "status": "ok",
     "timestamp": 1747653436052,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "Yis2s7j5Rzx7",
    "outputId": "e94ab16e-15da-47cb-9aca-07951eba25d7"
   },
   "outputs": [],
   "source": [
    "# Le dictionnaire des comptages : {mot: fréquence, ...}\n",
    "word_counts = tokenizer.word_counts\n",
    "\n",
    "# Total des occurrences dans le corpus\n",
    "total_occurrences = sum(word_counts.values())\n",
    "\n",
    "# Extraire les fréquences et les trier par ordre décroissant\n",
    "sorted_counts = sorted(word_counts.values(), reverse=True)\n",
    "\n",
    "# Calculer la somme cumulative\n",
    "cum_counts = np.cumsum(sorted_counts)\n",
    "\n",
    "# Calculer le pourcentage cumulatif\n",
    "cum_percent = cum_counts / total_occurrences * 100\n",
    "\n",
    "# Trouver le nombre de mots qui donnent au moins 95% de la couverture\n",
    "# np.argmax renvoie le premier indice où la condition est vraie\n",
    "num_words_95 = np.argmax(cum_percent >= 95) + 1  # +1 car l'indice commence à 0\n",
    "\n",
    "#print(f\"Pour atteindre 95% de la couverture, il faut environ {num_words_95} mots.\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(cum_percent)\n",
    "plt.xlabel(\"Nombre de mots (classés par fréquence)\")\n",
    "plt.ylabel(\"Pourcentage cumulatif\")\n",
    "plt.title(\"Distribution cumulée des fréquences des mots\\n(Pour atteindre 95% de la couverture, il faut environ {num_words_95} mots)\".format(num_words_95=num_words_95))\n",
    "plt.axhline(95, color='red', linestyle='--')\n",
    "plt.axvline(num_words_95, color='red', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(imgPrezPath, \"RNBrut95cover.svg\"),format=\"svg\",bbox_inches=\"tight\",pad_inches=0.1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 777,
     "status": "ok",
     "timestamp": 1747653436826,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "6a6WHG7jR2jd",
    "outputId": "22833df3-2f0b-41a0-ac55-dd2f04cd0088"
   },
   "outputs": [],
   "source": [
    "# Trouver le nombre de mots qui donnent au moins 95% de la couverture\n",
    "# np.argmax renvoie le premier indice où la condition est vraie\n",
    "num_words_90 = np.argmax(cum_percent >= 90) + 1  # +1 car l'indice commence à 0\n",
    "\n",
    "#print(f\"Pour atteindre 90% de la couverture, il faut environ {num_words_90} mots.\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(cum_percent)\n",
    "plt.xlabel(\"Nombre de mots (classés par fréquence)\")\n",
    "plt.ylabel(\"Pourcentage cumulatif\")\n",
    "plt.title(\"Distribution cumulée des fréquences des mots\\n(Pour atteindre 90% de la couverture, il faut environ {num_words_90} mots)\".format(num_words_90=num_words_90))\n",
    "plt.axhline(90, color='red', linestyle='--')\n",
    "plt.axvline(num_words_90, color='red', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(imgPrezPath, \"RNBrut90cover.svg\"),format=\"svg\",bbox_inches=\"tight\",pad_inches=0.1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpeQLmyZR76Q"
   },
   "source": [
    "je vais donc régler mon num_words à 10k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaTbu4N9SYU-"
   },
   "source": [
    "## Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 171,
     "status": "ok",
     "timestamp": 1747653436998,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "OLQMdOHfPueF",
    "outputId": "2886c404-3316-4564-92c7-b980021c8a6d"
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"RN_Brut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5226277,
     "status": "ok",
     "timestamp": 1747658663277,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "VTd8WaFTJyvr",
    "outputId": "e786e83a-b83a-41cf-fbbc-f1304477bc06"
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"GridSearch_sample_20pct\"):\n",
    "    best_model, best_params, best_val_f1, test_acc, test_f1, test_roc_auc, inf_time_ms = train_pipeline(X, y, num_words=10000, sample_frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1375760,
     "status": "ok",
     "timestamp": 1747660039024,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "WIiUFll0PtFX",
    "outputId": "98be4bff-45c1-48ae-d31e-4615a5c0c89f"
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Final_refit_full_data\"):\n",
    "    model, history, test_acc, test_f1, test_roc_auc, inf_time_ms = refit_best_model(X, y, num_words=10000, best_params=best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1747660039584,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "_RnLAKUCPKWD",
    "outputId": "b04f16ba-0acf-4389-88cd-87a34f050557"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "hist = history.history\n",
    "epochs = range(1, len(hist['loss']) + 1)\n",
    "# Loss\n",
    "plt.plot(epochs, hist['loss'],\n",
    "         label='Training loss',\n",
    "         linestyle='-',\n",
    "         linewidth=2,\n",
    "         color=bleufonce)\n",
    "plt.plot(epochs, hist['val_loss'],\n",
    "         label='Validation loss',\n",
    "         linestyle='--',\n",
    "         linewidth=2,\n",
    "         color=bleufonce)\n",
    "\n",
    "# Accuracy\n",
    "plt.plot(epochs, hist['accuracy'],\n",
    "         label='Training accuracy',\n",
    "         linestyle='-',\n",
    "         linewidth=2,\n",
    "         color=bleuclair)\n",
    "plt.plot(epochs, hist['val_accuracy'],\n",
    "         label='Validation accuracy',\n",
    "         linestyle='--',\n",
    "         linewidth=2,\n",
    "         color=couleur_complementaire)\n",
    "\n",
    "# Ajout de l’accuracy test en ligne horizontale\n",
    "plt.axhline(test_acc, linestyle=':', linewidth=2, color='darkgreen', label='Test accuracy')\n",
    "\n",
    "# Mise en forme\n",
    "plt.title(\"Évolution de la perte et de l'exactitude\", fontsize=14)\n",
    "plt.xlabel(\"Époque\", fontsize=12)\n",
    "plt.ylabel(\"Valeur\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(imgPrezPath, \"RNBrutRefitHistoryavecLR.svg\"), format=\"svg\", bbox_inches=\"tight\", pad_inches=0.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMeE3yOxArFh"
   },
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 360898,
     "status": "ok",
     "timestamp": 1747660400487,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "EE0leTFQj2-N"
   },
   "outputs": [],
   "source": [
    "#forming a separate feature for cleaned tweets\n",
    "for i,v in enumerate(df_cleaned['text']):\n",
    "  df_cleaned.loc[i,'text'] = p.clean(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1747660400497,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "yfwtgsXAj8_h"
   },
   "outputs": [],
   "source": [
    "X = df_cleaned['text']\n",
    "y = df_cleaned['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20763,
     "status": "ok",
     "timestamp": 1747660421210,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "ghk-tzKRkAFA",
    "outputId": "2d9c2f44-ebbe-4628-908c-aa52fbc2946d"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "total_mots = len(tokenizer.word_index)\n",
    "print(\"Nombre total de mots uniques :\", total_mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 917,
     "status": "ok",
     "timestamp": 1747660422077,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "axWlnrbjkDXv",
    "outputId": "63d9960e-8137-43ea-e056-4bd980b47279"
   },
   "outputs": [],
   "source": [
    "# Le dictionnaire des comptages : {mot: fréquence, ...}\n",
    "word_counts = tokenizer.word_counts\n",
    "\n",
    "# Total des occurrences dans le corpus\n",
    "total_occurrences = sum(word_counts.values())\n",
    "\n",
    "# Extraire les fréquences et les trier par ordre décroissant\n",
    "sorted_counts = sorted(word_counts.values(), reverse=True)\n",
    "\n",
    "# Calculer la somme cumulative\n",
    "cum_counts = np.cumsum(sorted_counts)\n",
    "\n",
    "# Calculer le pourcentage cumulatif\n",
    "cum_percent = cum_counts / total_occurrences * 100\n",
    "\n",
    "# Trouver le nombre de mots qui donnent au moins 95% de la couverture\n",
    "# np.argmax renvoie le premier indice où la condition est vraie\n",
    "num_words_95 = np.argmax(cum_percent >= 95) + 1  # +1 car l'indice commence à 0\n",
    "\n",
    "# print(f\"Pour atteindre 95% de la couverture, il faut environ {num_words_95} mots.\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(cum_percent)\n",
    "plt.xlabel(\"Nombre de mots (classés par fréquence)\")\n",
    "plt.ylabel(\"Pourcentage cumulatif\")\n",
    "plt.title(\"Distribution cumulée des fréquences des mots\\n(Pour atteindre 95% de la couverture, il faut environ {num_words_95} mots)\".format(num_words_95=num_words_95))\n",
    "plt.axhline(95, color='red', linestyle='--')\n",
    "plt.axvline(num_words_95, color='red', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(imgPrezPath, \"PreProc95cover.svg\"),format=\"svg\",bbox_inches=\"tight\",pad_inches=0.1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 816,
     "status": "ok",
     "timestamp": 1747660422891,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "VeQHH5fzkIHb",
    "outputId": "3596d627-d8fb-491e-81a7-b1baa3c1a0df"
   },
   "outputs": [],
   "source": [
    "# Trouver le nombre de mots qui donnent au moins 95% de la couverture\n",
    "# np.argmax renvoie le premier indice où la condition est vraie\n",
    "num_words_90 = np.argmax(cum_percent >= 90) + 1  # +1 car l'indice commence à 0\n",
    "\n",
    "# print(f\"Pour atteindre 90% de la couverture, il faut environ {num_words_90} mots.\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(cum_percent)\n",
    "plt.xlabel(\"Nombre de mots (classés par fréquence)\")\n",
    "plt.ylabel(\"Pourcentage cumulatif\")\n",
    "plt.title(\"Distribution cumulée des fréquences des mots\\n(Pour atteindre 90% de la couverture, il faut environ {num_words_90} mots)\".format(num_words_90=num_words_90))\n",
    "plt.axhline(90, color='red', linestyle='--')\n",
    "plt.axvline(num_words_90, color='red', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(imgPrezPath, \"PreProc90cover.svg\"),format=\"svg\",bbox_inches=\"tight\",pad_inches=0.1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w46S0CqinFRG"
   },
   "source": [
    "Je vais donc prendre 4k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmu8Nog4kPcV"
   },
   "source": [
    "## Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747660422902,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "h-rzUMe8kRKf"
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"RN_Preproc\")\n",
    "callbacks = create_callbacks(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5658350,
     "status": "ok",
     "timestamp": 1747666081261,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "WM4kK2JHlaMj",
    "outputId": "4afa7ea9-819c-414b-c75c-282e06230043"
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"GridSearch_sample_20pct_PP\"):\n",
    "    best_model_pp, best_params_pp, best_val_f1_pp, test_acc_pp, test_f1_pp, test_roc_auc_pp, inf_time_ms_pp = train_pipeline(X, y, num_words=4000, sample_frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 772283,
     "status": "ok",
     "timestamp": 1747666853540,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "pvKzuiFTlf3k",
    "outputId": "b6eb7de9-67d9-4892-9391-753513564a0c"
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Final_refit_full_data\"):\n",
    "    model_pp, history_pp, test_acc_pp, test_f1_pp, test_roc_auc_pp, inf_time_ms_pp = refit_best_model(X, y, num_words=4000, best_params=best_params_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "executionInfo": {
     "elapsed": 888,
     "status": "ok",
     "timestamp": 1747666854438,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "6xSFuy8zWJPW",
    "outputId": "afaf827a-3960-446a-cf0c-03563169ce16"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "hist_pp = history_pp.history\n",
    "epochs = range(1, len(hist_pp['loss']) + 1)\n",
    "# Loss\n",
    "plt.plot(epochs, hist_pp['loss'],\n",
    "         label='Training loss',\n",
    "         linestyle='-',\n",
    "         linewidth=2,\n",
    "         color=bleufonce)\n",
    "plt.plot(epochs, hist_pp['val_loss'],\n",
    "         label='Validation loss',\n",
    "         linestyle='--',\n",
    "         linewidth=2,\n",
    "         color=bleufonce)\n",
    "\n",
    "# Accuracy\n",
    "plt.plot(epochs, hist_pp['accuracy'],\n",
    "         label='Training accuracy',\n",
    "         linestyle='-',\n",
    "         linewidth=2,\n",
    "         color=bleuclair)\n",
    "plt.plot(epochs, hist_pp['val_accuracy'],\n",
    "         label='Validation accuracy',\n",
    "         linestyle='--',\n",
    "         linewidth=2,\n",
    "         color=couleur_complementaire)\n",
    "\n",
    "# Ajout de l’accuracy test en ligne horizontale\n",
    "plt.axhline(test_acc_pp, linestyle=':', linewidth=2, color='darkgreen', label='Test accuracy')\n",
    "\n",
    "# Mise en forme\n",
    "plt.title(\"Évolution de la perte et de l'exactitude\", fontsize=14)\n",
    "plt.xlabel(\"Époque\", fontsize=12)\n",
    "plt.ylabel(\"Valeur\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(imgPrezPath, \"RNPPRefitHistory.svg\"), format=\"svg\", bbox_inches=\"tight\", pad_inches=0.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDLQshfCAuVb"
   },
   "source": [
    "# 3. Preprocessing + Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1747666854682,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "SOzggSD84lc2",
    "outputId": "d2001b64-0798-439e-9d4f-c794d37f6347"
   },
   "outputs": [],
   "source": [
    "!nproc --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1747666855329,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "t22fZRJ-vW7m"
   },
   "outputs": [],
   "source": [
    "# Charger un modèle léger et désactiver tout sauf le tagger + lemmatizer\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1747666855383,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "QjXvlw8g4un7"
   },
   "outputs": [],
   "source": [
    "# Fonction pour lemmatiser en batches\n",
    "def spacy_tokenizer_texts(texts, batch_size=1000, n_process=4):\n",
    "    \"\"\"\n",
    "    Lemmatise et nettoie une liste de textes en lots, en parallèle.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): vos documents bruts.\n",
    "        batch_size (int): taille des lots à envoyer à spaCy à chaque appel.\n",
    "        n_process (int): nombre de processus (cœurs) à utiliser.\n",
    "    Returns:\n",
    "        list of str: textes tokenisés (lemmes en minuscules, sans stop‑words ni ponctuation).\n",
    "    \"\"\"\n",
    "    tokenized = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size, n_process=n_process):\n",
    "        tokens = [\n",
    "            token.lemma_.lower()\n",
    "            for token in doc\n",
    "            if token.is_alpha and not token.is_stop\n",
    "        ]\n",
    "        tokenized.append(\" \".join(tokens))\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4485068,
     "status": "ok",
     "timestamp": 1747671340429,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "LX-CcfmO42gB"
   },
   "outputs": [],
   "source": [
    "# Appliquer à toute la colonne en une seule passe + conversion en liste\n",
    "texts = df_cleaned['text'].tolist()\n",
    "df_cleaned['tokenized'] = spacy_tokenizer_texts(texts, batch_size=500, n_process=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1747671340666,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "NN_6N51l59xM",
    "outputId": "f46b04f3-1073-4d62-bf92-392d36084be3"
   },
   "outputs": [],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1747671340694,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "LPUs-dEaKpAn",
    "outputId": "a3d54d36-1b04-45b3-fe0a-2a7080f45461"
   },
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1747671340764,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "tQRAqlLkvgUv"
   },
   "outputs": [],
   "source": [
    "X = df_cleaned['tokenized']\n",
    "y = df_cleaned['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15313,
     "status": "ok",
     "timestamp": 1747671356082,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "RE02fJJmvljt",
    "outputId": "218bc1c5-3fb9-4216-fdeb-50bd9d6efe96"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Nombre total de mots uniques dans le corpus\n",
    "total_mots = len(tokenizer.word_index)\n",
    "print(\"Nombre total de mots uniques :\", total_mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 949,
     "status": "ok",
     "timestamp": 1747671357033,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "IiQQCwZPvqL0",
    "outputId": "b4eb828b-8e29-4eab-824d-0ae3dad845d7"
   },
   "outputs": [],
   "source": [
    "# Le dictionnaire des comptages : {mot: fréquence, ...}\n",
    "word_counts = tokenizer.word_counts\n",
    "\n",
    "# Total des occurrences dans le corpus\n",
    "total_occurrences = sum(word_counts.values())\n",
    "\n",
    "# Extraire les fréquences et les trier par ordre décroissant\n",
    "sorted_counts = sorted(word_counts.values(), reverse=True)\n",
    "\n",
    "# Calculer la somme cumulative\n",
    "cum_counts = np.cumsum(sorted_counts)\n",
    "\n",
    "# Calculer le pourcentage cumulatif\n",
    "cum_percent = cum_counts / total_occurrences * 100\n",
    "\n",
    "# Trouver le nombre de mots qui donnent au moins 95% de la couverture\n",
    "# np.argmax renvoie le premier indice où la condition est vraie\n",
    "num_words_95 = np.argmax(cum_percent >= 95) + 1  # +1 car l'indice commence à 0\n",
    "\n",
    "# print(f\"Pour atteindre 95% de la couverture, il faut environ {num_words_95} mots.\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(cum_percent)\n",
    "plt.xlabel(\"Nombre de mots (classés par fréquence)\")\n",
    "plt.ylabel(\"Pourcentage cumulatif\")\n",
    "plt.title(\"Distribution cumulée des fréquences des mots\\n(Pour atteindre 95% de la couverture, il faut environ {num_words_95} mots)\".format(num_words_95=num_words_95))\n",
    "plt.axhline(95, color='red', linestyle='--')\n",
    "plt.axvline(num_words_95, color='red', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(imgPrezPath, \"PreProcLemm95cover.svg\"),format=\"svg\",bbox_inches=\"tight\",pad_inches=0.1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 662,
     "status": "ok",
     "timestamp": 1747671357698,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "YoymBFLKnqK_",
    "outputId": "a95442f7-f578-48ec-d2b5-0d4e5dec91c0"
   },
   "outputs": [],
   "source": [
    "# Trouver le nombre de mots qui donnent au moins 95% de la couverture\n",
    "# np.argmax renvoie le premier indice où la condition est vraie\n",
    "num_words_90 = np.argmax(cum_percent >= 90) + 1  # +1 car l'indice commence à 0\n",
    "\n",
    "# print(f\"Pour atteindre 90% de la couverture, il faut environ {num_words_90} mots.\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(cum_percent)\n",
    "plt.xlabel(\"Nombre de mots (classés par fréquence)\")\n",
    "plt.ylabel(\"Pourcentage cumulatif\")\n",
    "plt.title(\"Distribution cumulée des fréquences des mots\\n(Pour atteindre 90% de la couverture, il faut environ {num_words_90} mots)\".format(num_words_90=num_words_90))\n",
    "plt.axhline(90, color='red', linestyle='--')\n",
    "plt.axvline(num_words_90, color='red', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(imgPrezPath, \"PreProcLemm90cover.svg\"),format=\"svg\",bbox_inches=\"tight\",pad_inches=0.1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5113325,
     "status": "ok",
     "timestamp": 1747676471026,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "7EgsZO11LSeh",
    "outputId": "11879165-784f-4513-aad3-695b3084f191"
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"RN_Preproc_Lemm\")\n",
    "with mlflow.start_run(run_name=\"GridSearch_sample_20pct_PPL\"):\n",
    "    best_model_ppl, best_params_ppl, best_val_f1_ppl, test_acc_ppl, test_f1_ppl, test_roc_auc_ppl, inf_time_ms_ppl = train_pipeline(X, y, num_words=8000, sample_frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 732809,
     "status": "ok",
     "timestamp": 1747677203839,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "VtaZ-QMM-xXZ",
    "outputId": "77aabd08-b9b8-4269-c1f2-5b352db3add1"
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Final_refit_full_data\"):\n",
    "    model_ppl, history_ppl, test_acc_ppl, test_f1_ppl, test_roc_auc_ppl, inf_time_ms_ppl = refit_best_model(X, y, num_words=8000, best_params=best_params_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "executionInfo": {
     "elapsed": 1052,
     "status": "ok",
     "timestamp": 1747677204894,
     "user": {
      "displayName": "Fabien Cappelli",
      "userId": "13665664238437409195"
     },
     "user_tz": -60
    },
    "id": "xvwsQdVOWleP",
    "outputId": "3be12a53-19ab-46eb-bdef-e02d8ef17b2c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "hist_ppl = history_ppl.history\n",
    "epochs = range(1, len(hist_ppl['loss']) + 1)\n",
    "# Loss\n",
    "plt.plot(epochs, hist_ppl['loss'],\n",
    "         label='Training loss',\n",
    "         linestyle='-',\n",
    "         linewidth=2,\n",
    "         color=bleufonce)\n",
    "plt.plot(epochs, hist_ppl['val_loss'],\n",
    "         label='Validation loss',\n",
    "         linestyle='--',\n",
    "         linewidth=2,\n",
    "         color=bleufonce)\n",
    "\n",
    "# Accuracy\n",
    "plt.plot(epochs, hist_ppl['accuracy'],\n",
    "         label='Training accuracy',\n",
    "         linestyle='-',\n",
    "         linewidth=2,\n",
    "         color=bleuclair)\n",
    "plt.plot(epochs, hist_ppl['val_accuracy'],\n",
    "         label='Validation accuracy',\n",
    "         linestyle='--',\n",
    "         linewidth=2,\n",
    "         color=couleur_complementaire)\n",
    "\n",
    "# Ajout de l’accuracy test en ligne horizontale\n",
    "plt.axhline(test_acc_ppl, linestyle=':', linewidth=2, color='darkgreen', label='Test accuracy')\n",
    "\n",
    "# Mise en forme\n",
    "plt.title(\"Évolution de la perte et de l'exactitude\", fontsize=14)\n",
    "plt.xlabel(\"Époque\", fontsize=12)\n",
    "plt.ylabel(\"Valeur\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(imgPrezPath, \"RNPPLRefitHistory.svg\"), format=\"svg\", bbox_inches=\"tight\", pad_inches=0.1)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOHdIRvEhJvl15bI9UQeLVo",
   "gpuType": "T4",
   "mount_file_id": "1M-kQyvN5bsVhTicannxSirr7drAoq3XN",
   "provenance": [
    {
     "file_id": "1em6wfNnZ5JG-kRwCA7pMq_c1fyf2-YHu",
     "timestamp": 1747397199618
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
